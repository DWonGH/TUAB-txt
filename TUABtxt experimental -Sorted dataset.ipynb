{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUABtxt experimental - updated database.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfSvVcDo6GQ"
      },
      "source": [
        "# Load text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwlfPb11GH8J"
      },
      "source": [
        "This notebook demonstrates the classification of EEG text reports from the Temple University Hospital EEG Corpus. The basic code structure is based on Example 1 in [this demo](https://www.tensorflow.org/tutorials/load_data/text).\n",
        "\n",
        "First, let's install and import some useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa6IKWvADqH7"
      },
      "source": [
        "# Be sure you're using the stable versions of both tf and tf-text, for binary compatibility.\n",
        "!pip install -q -U tensorflow\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "source": [
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjC3yLa5IjP7"
      },
      "source": [
        "# Download and explore the dataset\n",
        "\n",
        "First we'll use a handy tool called `gdown` to download the dataset (just the text reports) from where your team have stored them on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOkBFWym_Ted",
        "outputId": "29cab328-3f10-4f16-857f-3ff5d044a7f7"
      },
      "source": [
        "!gdown --id 1NuNQw_HT49c0Omb051xko1NvGTh1m0lx"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NuNQw_HT49c0Omb051xko1NvGTh1m0lx\n",
            "To: /content/TUAB_txt_sorted.tar\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 30.5MB/s]\r9.37MB [00:00, 43.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9AqWm9N6fQ4"
      },
      "source": [
        "The dataset is compressed inside the archive file TUABtxt.tar, so let's extract it (like unzipping a zip file)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g58H2ZGZBG1R"
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"TUAB_txt_sorted.tar\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsaNjolR6qpU"
      },
      "source": [
        "Now we've extracted a folder called TUABtxt. Let's use pathlib library to explore this directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKBIa3GkCPd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993931fa-55f4-40ba-ecfb-d74331ae9a76"
      },
      "source": [
        "dataset_dir = pathlib.Path('TUAB_txt_sorted/v2.0.0/edf')\n",
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('TUAB_txt_sorted/v2.0.0/edf/train'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/eval')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbMpvH7KV-I6"
      },
      "source": [
        "Extract the training dataset from the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7h9xrPAGOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6925406-c255-4246-e314-001e29c50abe"
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/normal'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNJKTjPdU5a"
      },
      "source": [
        "Extract the Evaluation dataset from the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wChRhNrdUJu",
        "outputId": "fc83da41-41b3-4b33-aae0-b69a3c95ae62"
      },
      "source": [
        "eval_dir = dataset_dir/'eval'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/normal'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQabHmde6t-3"
      },
      "source": [
        "From the training dataset extract the abnormal and normal classes. Abnormal Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEoV7YByJoWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675e60cd-f0d6-44b7-99b6-2bb106add9d2"
      },
      "source": [
        "abnormal_train_dir = train_dir/'abnormal/01_tcp_ar'\n",
        "list(abnormal_train_dir.iterdir())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/081'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/049'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/012'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/007'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/101'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/032'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/008'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/080'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/086'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/027'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/039'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/016'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/062'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/096'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/068'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/095'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/093'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/076'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/074'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/078'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/094'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/033'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/004'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/103'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/031'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/104'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/009'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/042'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/070'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/057'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/083'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/107'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/029'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/005'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/038'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/073'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/021'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/108'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/064'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/040'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/000'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/072'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/061'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/098'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/001'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/013'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/085'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/055'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/053'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/100'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/097'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/079'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/069'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/010'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/088'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/106'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/056'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/065'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/030'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/003'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/017'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/071'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/067'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/015'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/018'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/006'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/084'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/089'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/014'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/022'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/092'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/102'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/047'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/105'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/090'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/075'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/099'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/052'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/043'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/046'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/025'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/051'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/023'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/059'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/050'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/054'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/048'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/091'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/044'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/037'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/028'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/077'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/011'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/020'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/035'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/041'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/066'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/082'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/002'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/087'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/036'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/058'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/024'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/060'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/063')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvf7VqZvWdMz"
      },
      "source": [
        "Normal Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU8S29wuWc6J",
        "outputId": "73226a8c-6442-440d-a0bd-52ab2ef27cad"
      },
      "source": [
        "normal_train_dir = train_dir/'normal/01_tcp_ar'\n",
        "list(abnormal_train_dir.iterdir())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/081'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/049'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/012'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/007'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/101'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/032'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/008'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/080'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/086'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/027'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/039'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/016'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/062'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/096'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/068'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/095'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/093'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/076'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/074'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/078'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/094'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/033'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/004'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/103'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/031'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/104'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/009'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/042'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/070'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/057'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/083'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/107'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/029'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/005'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/038'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/073'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/021'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/108'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/064'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/040'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/000'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/072'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/061'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/098'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/001'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/013'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/085'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/055'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/053'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/100'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/097'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/079'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/069'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/010'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/088'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/106'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/056'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/065'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/030'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/003'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/017'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/071'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/067'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/015'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/018'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/006'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/084'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/089'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/014'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/022'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/092'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/102'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/047'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/105'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/090'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/075'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/099'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/052'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/043'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/046'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/025'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/051'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/023'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/059'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/050'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/054'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/048'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/091'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/044'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/037'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/028'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/077'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/011'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/020'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/035'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/041'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/066'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/082'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/002'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/087'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/036'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/058'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/024'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/060'),\n",
              " PosixPath('TUAB_txt_sorted/v2.0.0/edf/train/abnormal/01_tcp_ar/063')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C21uueJ5fahX"
      },
      "source": [
        "Evaluation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYtfOZlRdtz7",
        "outputId": "7f998394-77b8-4678-8b19-14419d30cdd1"
      },
      "source": [
        "abnormal_eval_dir = eval_dir/'abnormal/01_tcp_ar'\n",
        "print(len(list(abnormal_eval_dir.iterdir())))\n",
        "normal_eval_dir = eval_dir/'normal/01_tcp_ar'\n",
        "print(len(list(normal_eval_dir.iterdir())))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52\n",
            "57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mxAN17MhEh0"
      },
      "source": [
        "We see from the above output that the data is stored across many subfolders. The documentation for the TUAB set explains this folder structure. Below each of the arbitrary subfolders listed above is a further hierarchy a folders for individual subjects and recording sessions. You don't need to understand this structure in detail, because we'll use a function to automatically extract the txt data. But let's just take a look inside one of the txt files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go1vTSGdJu08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbf5559-6c90-4481-cfd1-3b8a74b2af30"
      },
      "source": [
        "sample_file = abnormal_train_dir/'007/00000739/s002_2012_09_26/00000739_s002.txt'\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLINICAL HISTORY:  A 64-year-old male found with an empty bottle of Januvia, and glucose of 99.  Given Narcan, Ativan and moxifloxacin.  He has a history of diabetes and neuropathy.  Previous EEG in 2003 is not available.\n",
            "MEDICATIONS:  Ativan, Narcan, others.\n",
            "INTRODUCTION:  Digital video EEG was performed at the bedside in the ICU using standard 10-20 system of electrode placement with one channel of EKG.  The patient was poorly responsive.\n",
            "DESCRIPTION OF THE RECORD:  The background EEG demonstrates a mixed frequency background with spontaneous arousals and generous beta.\n",
            "The patient drifts off to sleep and there is an increase in background slowing.  Occasional FIRDA was noted.\n",
            "IMPRESSION:  This is an abnormal EEG due to:  Generalized background slowing.\n",
            "CLINICAL CORRELATION:  The generous beta described above may be due to this patient's medications.  If epilepsy is an important consideration a follow up study when the patient is out of the ICU may be helpful.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deWBTkpJiO7D"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Next, we will load the data off disk and prepare it into a format suitable for training. The text_dataset_from_directory utility makes this easy, and creates a tf.data.Dataset object with labels ('normal' and 'abnormal') automatically recognised from the folder structure. (tf.data is a collection of tools for building input pipelines for machine learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts9MdI-lAa-X",
        "outputId": "febcf252-61b2-41f0-fc6c-7ef6519ed041"
      },
      "source": [
        "full_train_ds = preprocessing.text_dataset_from_directory(train_dir, batch_size=16)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2717 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Z62emUNv5w",
        "outputId": "d76f75de-e91f-4497-83e3-d12457b85422"
      },
      "source": [
        "test_ds = preprocessing.text_dataset_from_directory(eval_dir, batch_size=6)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 276 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyl6JTAjlbQV"
      },
      "source": [
        "When running a machine learning experiment, it is a best practice to divide your dataset into three splits: [train](https://developers.google.com/machine-learning/glossary#training_set), [validation](https://developers.google.com/machine-learning/glossary#validation_set), and [test](https://developers.google.com/machine-learning/glossary#test-set). There are no strict rules, but usually it's best to put most of your data in the training (so that there's plenty to learn from. A 70-15-15 percent split is fairly common, as implemented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s96Dk4H-AfAd",
        "outputId": "bb349b75-6849-4b4c-89da-dc23fd4f5d0b"
      },
      "source": [
        "# Set the size of each subset of data:\n",
        "n = len(list(full_train_ds)) # Number of batches in original dataset\n",
        "n_train = int(0.7*n)   # Use about 70% as training data ...\n",
        "n_val = int(0.15*n)    # ... 15% as validation data ...\n",
        "n_test = n-n_train-n_val # ... and the rest as test data.\n",
        "print(f\"We have {n} batches in the full dataset.\")\n",
        "print(f\"We'll use {n_train} batches in the training set, {n_val} in the validation set, and {n_test} in the test set.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 170 batches in the full dataset.\n",
            "We'll use 118 batches in the training set, 25 in the validation set, and 27 in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkJwll9lT6Ik"
      },
      "source": [
        "Now we're ready to actually make the split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkpD8UquD0kO"
      },
      "source": [
        "# Split the data into training, validation, and test sets:\n",
        "raw_train_ds = full_train_ds.take(n_train)\n",
        "raw_val_ds = full_train_ds.skip(n_train).take(n_val)\n",
        "raw_test_ds = full_train_ds.skip(n_train+n_val)\n",
        "#raw_test_ds = test_ds\n",
        "\n",
        "assert(len(list(raw_test_ds))==n_test) # This assertion statement checks our code, to make sure the test dataset size is what we expect."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMI_gPLfloD7"
      },
      "source": [
        "Let's print out a few examples, to get more of a feel for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JMTyZ6Glt_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c280dba-3286-4437-bd8c-b901a42f0e6f"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):   # Take a single batch from the dataset.\n",
        "  for i in range(10):                                  # Iterate through the first 10 examples in that batch.\n",
        "    print(\"Report: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report:  b'CLINICAL HISTORY: 29 year old female with cerebral palsy, multiple seizures, recent PEG tube placement due to aspiration pneumonia. This is an outpatient EEG.\\nMEDICATIONS: Dilantin, Topamax, Phenobarbital\\nINTRODUCTION: Digital video EEG was performed in lab using standard 10-20 system of electrode placement with 1 channel of EKG.\\nDESCRIPTION OF THE RECORD: The overall background is poorly organized. There is rhythmic, 5.3 Hz activity noted in the posterior regions on the right. This activity is somewhat faster on the left at 5.8 Hz. The overall background is high amplitude, poorly organized. Paroxysmal bursts of generalized 6 Hz activity are seen intermittently in the background. As the patient becomes drowsy, these bursts are a bit more prominent. There are rare left frontocentral sharp waves.\\nHR: 120 bpm\\nIMPRESSION: Abnormal EEG due to:\\n1. Background slowing.\\n2. Slowing of the alpha rhythm.\\n3. An asymmetry in alpha rhythm with a slower rhythm on the right.\\n4. Paroxysmal bursts of rhythmic 5 Hz to 6 Hz activity.\\n5. Rare left frontal sharp waves.\\n\\nCLINICAL CORRELATION: These findings support an underlying encephalopathy. The types of abnormalities described above are seen in a chronic static encephalopathy. No seizures were recorded. If additional information is needed, a repeat study capturing deeper stages of sleep can be of value.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b'CLINICAL HISTORY:  37 year old right handed woman with epilepsy, currently seizure free.\\nMEDICATIONS:  Tegretol, Levothyroxine\\nINTRODUCTION:  Digital video EEG was performed in lab using standard 10-20 system of electrode placement with 1 channel EKG.  Hyperventilation and photic stimulation were performed.\\nDESCRIPTION OF THE RECORD:  In wakefulness there is a 9 Hz rhythm.  Hyperventilation produces a prominent hyperventilation response with background rhythmic theta and delta.  In addition, there is a focal hyperventilation response with occipitally predominant, sharply activity and activity that is periodic and sharply contoured in the right occipital region.  As the patient becomes drowsy, the focal slowing and sharp waves continue in the right occipital region, but then there is more of a transition to a drowsy pattern with a more rhythmic symmetric pattern.  Focal slowing and some arrhythmic delta are noted in the right occipital region.   Photic stimulation elicits little in the way of a driving response.\\nHR:    96 bpm\\nIMPRESSION:  Abnormal EEG due to:\\nIrregular focal slowing in the right occipital region.\\nRight occipital or posterior sharp waves.\\nA posteriorly predominant hyperventilation response.\\nCLINICAL CORRELATION:  These findings are similar to a previous study, but the previous study demonstrated nearly continuous occipital spiking.  These EEG has a bit more in the way of focal slowing.  Correlation with imaging is important.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b'REASON FOR STUDY:\\nCLINICAL HISTORY:\\nMEDICATIONS:\\nINTRODUCTION:  A routine EEG was performed using the standard 10-20 electrode placement system with additional anterior temporal and single-lead EKG electrode.  The patient was recorded during wakefulness and stage 1 sleep.  Activating procedures were not performed.\\nDESCRIPTION OF THE RECORD:  The record opens to a well-defined posterior dominant rhythm of 9-10 Hz which is reactive to eye opening.  There is normal frontocentral beta.  No activating procedures were performed.  The patient was recorded in wakefulness and stage 1 sleep.\\nABNORMAL DISCHARGES:  Focal slow waves at T3 and independently at T4 seen frequently throughout the recording at frequency of 2-4 Hz with an amplitude of 20-60 microvolts.  At times, this focal slowing can be very suspiciously sharply contoured, especially over the right hemisphere, but not clearly epileptic.\\nSEIZURES:  None.\\nIMPRESSION:  Abnormal EEG due intermittent independent bilateral midtemporal slowing.\\nCLINICAL CORRELATION:  This EEG reveals evidence of focal cerebral dysfunction seen independently in both midtemporal regions, which is nonspecific with regards to etiology.  Please note that at times some of the slowing, especially on the right side, was suspiciously sharply contoured and a repeated recording with __________ for the\\ndeeper stage of sleep may help in elucidating more clear diagnosis.  No seizures were seen.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b'CLINICAL HISTORY:  This is a 29-year-old male with endocarditis, status post code last night.  The patient had prolonged cardiac arrest, required temporary pacer and then developed myoclonic jerks today.  Now requiring dialysis.\\nMEDICATIONS:  Fentanyl, Versed, fosphenytoin, Ativan.\\nINTRODUCTION:  Digital video EEG was performed at the bedside in the ICU using standard 10-20 system of electrode placement with one channel of EKG.  The patient was intubated, comatose and had occasional jerks.\\nDESCRIPTION OF THE RECORD:  The EEG pattern demonstrates a burst suppression pattern.  The overall pattern is suppressed with some ICU artifact.  At a sensitivity of 3 there is scant low voltage activity and ICU artifact. The bursts of activity are typically 1.5 seconds in duration and include muscle activity, some fast activity and a paroxysmal burst of slow at the beginning and one at the offset.\\nStimulation of the patient did not activate the record.\\nHeart rate 84 beats per minute.\\nIMPRESSION:  This is a markedly abnormal EEG due to:\\nBurst suppression pattern with markedly suppressed activity.\\nBursts which occur approximately every 10 seconds lasting 1-3 seconds in duration.\\nCLINICAL CORRELATION:  No clinical seizures were noted.  No spontaneous reactivity or variability were observed.  In this individual who is more than 12 hours post cardiac arrest, these findings carry a very worrisome prognosis.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b\"CLINICAL HISTORY: 33 year old right handed male status post resection of glioma\\nwith past history of aphasia and hematoma.\\nMEDICATIONS: Keppra, Levetiracetam\\nINTRODUCTION: Digital video EEG was performed in lab using standard 10-20\\nsystem of electrode placement with 1 channel EKG. This is an awake and drowsy\\nrecord. This was a 1 hour sleep deprived EEG.\\nDESCRIPTION OF THE RECORD: On wakefulness, there is a 10.5 Hertz alpha rhythm\\nidentified bilaterally. There is a muscle artifact picked up with scalp electrode artifact at T3 almost continuously. There is also a bit of a breach with accentuation of beta\\nparticularly in the frontal regions. In hyperventilation there is a bit of focal slowing\\npicked up primarily in the left parietal rhythm followed by rhythmic frontal theta.\\nProlonged stage 2 sleep includes vertex waves and spindles. The patient's breach\\nrhythm is quite apparent there with higher amplitude activity in the left central region.\\nPhotic stimulation produces scant bilateral driving.\\nHR: 66 bpm\\nIMPRESSION: Abnormal EEG due to:  1. Left parietal subtle focal slowing.\\nCLINICAL CORRELATION: The subtle left frontal features were not identified today. No epileptiform features were noted. If epilepsy is an important consideration, a repeat tracing may be of use, but in any case the most recent EEGs have been without\\nepileptiform activity.\\n\\n\\n\"\n",
            "Label: 0\n",
            "Report:  b'CLINICAL HISTORY:   51 year old male with a subdural hematoma, right frontoparietal craniotomy, and 3 episodes of left facial twitching without confusion.  History of deafness and alcohol abuse.\\nMEDICATIONS:  Keppra, Lovenox, Zofran\\nINTRODUCTION:  Digital video EEG was performed at bedside using standard 10-20 system of electrode placement with 1 channel EKG.  The patient is cooperative.\\nDESCRIPTION OF THE RECORD:  The background EEG is medium amplitude and includes a mixture of theta and some delta.  There is also some alpha frequency activity.  There is some very subtle periodic slowing or sharply contoured sign with a paroxysmal slow wave noted in the right hemisphere.  There is a great deal of variability with some sections of the record much richer in alpha and theta frequency activity than others.  The very slow, subtle periodic epileptiform activity tends to occur in the sections of the record with more slowing.\\nHR:    72 to 90 bpm\\nIMPRESSION:  Abnormal EEG due to:\\nIntermittent, at times generalized, slowing.\\nSharply contoured paroxysmal slowing from the right temporal region.\\nCLINICAL CORRELATION:  No seizures were recorded.  This EEG is compatible with a structural process on the right.  There is a very subtle, broadly contoured burst of\\nparoxysmal slowing which is potentially epileptiform.  No seizures were recorded.  The reactivity and variability are positive features in an ICU patient.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b'CLINICAL HISTORY: 46 year old woman with a 4 cm posterior fossa hematoma and altered mentation.\\nMEDICATIONS: Dilantin, Percocet, Protonix\\nINTRODUCTION: Digital video EEG was performed at bedside using standard 10-20 system of electrode placement with 1 channel of EKG. The patient is intubated.\\nDESCRIPTION OF THE RECORD: The background EEG demonstrates a slow pattern with a mixture of theta and delta. The overall activity is relatively rhythmic and includes approximately 1 Hz delta, 3.5 Hz delta, and 4.5 Hz activity. In addition, there is some eye movement but there is also a subtle, right frontal sharp wave noted. FIRDA is also observed and is present bilaterally. Stimulation of the patient seems to produce more of a theta pattern than the frontal delta. Vigorous stimulation is not performed.\\nHR: 120 bpm\\nIMPRESSION: Abnormal EEG due to.\\n1. Replacement of normal background with the pattern of rhythmic slowing.\\n2. Right frontal sharp waves.\\n3. Frontal intermittent rhythmic delta activity.\\nCLINICAL CORRELATION: These findings may be correlated with imaging and clinical history. This EEG supports a diffuse disturbance of cerebral function in that there are frontal sharp waves, additional recording may be appropriate.\\n\\n\\n\\n'\n",
            "Label: 0\n",
            "Report:  b\"REASON FOR STUDY:  Differential and classification of seizures.\\nCLINICAL HISTORY:  A 48-year-old male with history of drug abuse, head trauma and a car accident in the 1980's presents to the lab after having seizures since the 1980's, sometimes asleep, described as shaking, also sleep walks.  Last seizure a month ago with loss of consciousness.  The patient is here for differential and classification of seizures in the Epilepsy Monitoring Unit.\\nTECHNICAL DIFFICULTIES:  None.\\nMEDICATIONS:  Keppra, Depakote, Topamax and Dilantin.\\nINTRODUCTION:  A routine EEG was performed using the standard 10-20 electrode placement system with additional temporal and single lead EKG electrode.  The patient was recorded during wakefulness and sleep.  Activating procedures included hyperventilation and photic stimulation.\\nDESCRIPTION OF THE RECORD:  The record appears to have a low amplitude posterior dominant rhythm that reaches 9 Hz which is reactive to eye opening.  There is excessive beta seen throughout the recording.  Activating procedures including hyperventilation and photic stimulation produced no abnormal discharges.  The patient is recorded in wakefulness and stage I sleep.\\nABNORMAL DISCHARGES:  None.\\nSEIZURES:  None.\\nHEART RATE:  84.\\nIMPRESSION:  Normal awake and asleep EEG.\\nCLINICAL CORRELATION:  This is a normal awake and sleep EEG.  No seizures or epileptiform discharges were seen.\\n\\n\\n\\n\"\n",
            "Label: 1\n",
            "Report:  b\"CLINICAL HISTORY:  A 25-year-old male status post a brain tumor resection in childhood, status epilepticus March 23, 2013 associated with cerebritis/stroke.  This is an awake and drowsy record in a cooperative patient.\\nMEDICATIONS:  Depakote, Vimpat.\\nINTRODUCTION:  Digital video EEG is performed in the lab/bedside using standard 10-20 system of electrode placement with one channel of EKG.  Hyperventilation and photic stimulation are performed.\\nDESCRIPTION OF THE RECORD:  In wakefulness, the background EEG demonstrates an asymmetry between the 2 hemispheres.  The right hemisphere includes an 8 Hz alpha rhythm.  The left hemisphere demonstrates some underlying focal slowing in the left posterior temporal region with a breach rhythm, accentuation of amplitude in that region, some underlying delta.  Drowsiness is characterized by an increase in background slowing.\\nIn the context of a breach rhythm, there is an underlying, P3/T5 sharp waves, primarily noted at P3.\\nPhotic stimulation does not significantly activate the record.  Hyperventilation produces some bilateral slowing.  Frontally predominant rhythmic theta was noted in hyperventilation.\\nHeart rate 78 bpm.\\nIMPRESSION:  This is an abnormal EEG due to:\\nBackground slowing noted from both hemispheres, the left more so than the right.\\nSubtle underlying delta noted in the left hemisphere, particularly in the posterior quadrant.\\nBreach rhythm.\\nSubtle underlying left parietal sharp waves in the context of a breech rhythm.\\nCLINICAL CORRELATION:  For this individual with a structural lesion and previous status epilepticus, this EEG is improved compared to other studies.  The patient is clearly awake with a well-formed alpha rhythm on the right, although it is a bit slow.  The focal features described above are compatible with the patient's known structural abnormality.  No seizures were recorded.\\n\\n\\n\\n\"\n",
            "Label: 0\n",
            "Report:  b'CLINICAL HISTORY: 22 year old woman with epilepsy, last seizure in 07/2010.\\nMEDICATIONS: Dilantin\\nINTRODUCTION: Digital video EEG was performed in lab using standard 10-20\\nsystem of electrode placement with 1 channel EKG. Hyperventilation and photic stimulation are performed.\\nDESCRIPTION OF THE RECORD: The background EEG demonstrates a youthful\\npattern with a high amplitude 10 Hertz alpha rhythm, occasional posterior slow waves of\\nyouth. Drowsiness is characterized by still rolling eye movements with an increasing\\nbeta. Hyperventilation produces a recent amplitude. Photic stimulation elicits driving.\\nHR: 64 bpm and slightly irregular\\nIMPRESSION: EEG within normal limits.\\nCLINICAL CORRELATION: No epileptiform features are observed. Normal EEG does\\nnot exclude a diagnosis of epilepsy.\\n\\n\\n\\n'\n",
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZGl4Q5l2sS"
      },
      "source": [
        "The labels are `0` or `1`. To see which of these correspond to which string label, you can check the `class_names` property on the dataset, as below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIpCS7YjmGkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad640310-1b56-44af-e84a-e531dff5fc84"
      },
      "source": [
        "for i, label in enumerate(full_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to abnormal\n",
            "Label 1 corresponds to normal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdt-ATrGRGDL"
      },
      "source": [
        "### Prepare the dataset for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fRti45Rlj8"
      },
      "source": [
        "Next, you will standardize, tokenize, and vectorize the data using the `preprocessing.TextVectorization` layer.\n",
        "* Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.\n",
        "\n",
        "* Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).\n",
        "\n",
        "* Vectorization refers to converting tokens into numbers so they can be fed into a neural network.\n",
        "\n",
        "All of these tasks can be accomplished with this layer. You can learn more about each of these in the [API doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
        "\n",
        "* The default standardization converts text to lowercase and removes punctuation.\n",
        "\n",
        "* The default tokenizer splits on whitespace.\n",
        "\n",
        "* The default vectorization mode is `int`. This outputs integer indices (one per token). This mode can be used to build models that take word order into account. You can also use other modes, like `binary`, to build bag-of-word models.\n",
        "\n",
        "\n",
        "Here we will use the `binary` mode to build a bag-of-words model (essentially one-hot encoding of whether each word in the vocabulary appears in the report). Then we will use the `int` mode (integer encoding of each word in the report, with order preserved) with a 1D ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voaC43rZR0jc"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDPFxuf2Hfz"
      },
      "source": [
        "For `int` mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length, which will cause the layer to pad or truncate sequences to exactly sequence_length values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWsY01Zl2aRe"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts6h9b5atD-Y"
      },
      "source": [
        "Next, you will call `adapt` to make the VectorizationLayer adjust itself according to the vocabulary in the dataset.\n",
        "\n",
        "Note: it's important to only use your training data when calling adapt (using the test set would leak information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTXsdDEqSf9e"
      },
      "source": [
        "# To avoid some errors caused by non-standard characters, we create a function\n",
        "# that does some additional 'cleaning' of the text.\n",
        "def clean_text(text, labels):\n",
        "  cleaned_version_of_text = tf.strings.unicode_transcode(text, \"US ASCII\", \"UTF-8\") \n",
        "  return cleaned_version_of_text\n",
        "  \n",
        "# Now apply our clean_text function to the full dataset.\n",
        "train_text = raw_train_ds.map(clean_text) \n",
        "\n",
        "# Finally, let the vectorize layers adjust themselves to fit the vocabulary of the dataset.\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVO6Jg7Sls0"
      },
      "source": [
        "See the result of using these layers to preprocess data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RngfPyArSsvM"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1W54wf0LhQ0"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi_sElMiSmXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20db0c23-ceb7-4114-fb54-4d20527b1004"
      },
      "source": [
        "# Retrieve a batch (of 32 reports and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_report, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Report\", first_report)\n",
        "print(\"Label\", first_label)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report tf.Tensor(b'REASON FOR STUDY:  Episodes of seizures.\\nCLINICAL HISTORY:  A 30-year-old man with history of seizures for 3 months, approximately once a month.  Described as single tingling, then right upper extremity numbness and stiffness with loss of consciousness and confusion afterwards.  Past medical history of dementia.\\nMEDICATIONS:  Tegretol and Seroquel.\\nINTRODUCTION:  A routine EEG is performed using standard 10-20 electrode placement with an anterior temporal and single lead EKG electrode.  The patient was recorded in wakefulness and sleep.  Activating procedures included hyperventilation and photic stimulation.\\nDESCRIPTION OF THE RECORD:  The record opens to a well-defined posterior dominant rhythm that reaches 9-10 Hz, which is reactive to eye opening.  There is normal frontocentral beta.  The patient is recorded during wakefulness, stage I and stage II sleep.  Please note, the patient easily falls asleep throughout the record.\\nActivating procedures produced no abnormal discharges.\\nABNORMAL DISCHARGES:  None.\\nSEIZURES:  None.\\nHEART RATE:  66.\\nIMPRESSION:  Normal awake and asleep electroencephalogram.\\nCLINICAL CORRELATION:  This is a normal awake and asleep EEG.  No seizures or epileptiform discharges are seen.\\n\\n\\n\\n', shape=(), dtype=string)\n",
            "Label tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGukZoYv2v3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c4f533-c603-44cb-8227-e72524a70eb5"
      },
      "source": [
        "print(\"'binary' vectorized report:\", \n",
        "      binary_vectorize_text(first_report, first_label)[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'binary' vectorized report: tf.Tensor([[0. 1. 1. ... 0. 0. 0.]], shape=(1, 6206), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu07FsIw2yH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7516c0e-968f-4502-b8dd-dcf7805a2def"
      },
      "source": [
        "print(\"'int' vectorized report:\",\n",
        "      int_vectorize_text(first_report, first_label)[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'int' vectorized report: tf.Tensor(\n",
            "[[ 150   54  103  208    3   23   12   18    6 1777  212    7   18    3\n",
            "    23   54  183  520  293 1076    6  558  215   64  131 1066  238   24\n",
            "   979 1162  571    4 1910    7  179    3  216    4  243 1148  182  271\n",
            "    18    3  549   34  431    4  785   49    6  190    9    5   13   51\n",
            "    44   45   29   52    7   25   92   53    4  131  148   37   29    2\n",
            "    17   11   98    8   35    4   20  108  107  240   30    4   21   19\n",
            "    41    3    2   14    2   14  149   10    6  265   62   91   32   88\n",
            "   195  297   40   70    5  146   10  105  125   16    5   36  127   61\n",
            "     2   17    5   98   81   35   65  167    4   65  134   20  365  284\n",
            "     2   17 1351 1275  161  189    2   14  108  107  248   22   46   58\n",
            "    46   58   75   23   75  117  124  294   42   36   85    4  161  532\n",
            "    12   48   15    5    6   36   85    4  161    9   22   23   68   50\n",
            "    58   33   57    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgjeF9PdS7tN"
      },
      "source": [
        "As you can see above, `binary` mode returns an array denoting which tokens exist at least once in the input, while `int` mode replaces each token by an integer, thus preserving their order. You can lookup the token (string) that each integer corresponds to by calling `.get_vocabulary()` on the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpBnTZilS8wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff87bbc-8963-45c9-9cf9-f7c2b2cb26bf"
      },
      "source": [
        "print(\"42 ---> \", int_vectorize_layer.get_vocabulary()[42])\n",
        "print(\"44 ---> \", int_vectorize_layer.get_vocabulary()[44])\n",
        "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42 --->  impression\n",
            "44 --->  standard\n",
            "Vocabulary size: 6215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHgPE_YwHvp"
      },
      "source": [
        "You are nearly ready to train your model. As a final preprocessing step, you will apply the `TextVectorization` layers you created earlier to the train, validation, and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46LeHmnD55wJ"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T4PaCEcNo9m"
      },
      "source": [
        "# Rule-Based (non-ML) Approach\n",
        "\n",
        "Looking through the reports, it seems as though it's usually stated quite clearly when the EEG is abnormal. Rather than attempting any machine learning, why don't we just look for that key word (or related words/phrases) in the text? A very basic version of this approach is implemented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WmU1JIHYKvr",
        "outputId": "74e64b75-35f1-43f1-c3ba-036f1fa5d1d7"
      },
      "source": [
        "# First initialise some counters\n",
        "n = 0\n",
        "n_correct = 0\n",
        "n_failed_decode = 0\n",
        "wrong_normal = 0\n",
        "wrong_abnormal = 0\n",
        "count = 0\n",
        "\n",
        "# Iterate over all batches, taking the text and labels batch-by-batch.\n",
        "# N.B. take(-1) has the effect of pulling out all the batches, instead of a specific number, as explained in the docs here: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take\n",
        "for text_batch, label_batch in full_train_ds.take(-1):\n",
        "\n",
        "  # Iterate over the report examples in the batch:\n",
        "  for ind,text in enumerate(text_batch):\n",
        "\n",
        "\n",
        "    # Get rid of any pesky non-standard characters using the function we created previously.\n",
        "    cleaned_text = clean_text(text,0)\n",
        "    # Then convert it from a tensorflow Tensor to a python string so that we can \n",
        "    # use some standard python text analysis on it.\n",
        "    cleaned_and_decoded_text = cleaned_text.numpy().decode(\"UTF-8\")\n",
        "\n",
        "    find_impression = re.search(\"impression\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\n",
        "    first_char,last_char = find_impression.span()\n",
        "\n",
        "    #find_clinical = re.search(\"clinical correlation:\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\n",
        "    #if find_clinical != None:\n",
        "    #  first_char_clin, last_char_clin = find_clinical.span()\n",
        "    #  last_char = first_char_clin\n",
        "    #else:\n",
        "    last_char = last_char +75\n",
        "\n",
        "    searched_text = cleaned_and_decoded_text.lower()[first_char:last_char]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    is_abnormal = re.search('abnormal|absence of normal|outside of the range of normal|not normal', searched_text.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "    is_normal = re.search('normal', searched_text.lower(), flags=re.IGNORECASE)\n",
        "    #also_abnormal = re.search(\"absence of normal\", searched_text.lower(), flags=re.IGNORECASE)\n",
        "    \n",
        "    # Check if the word 'abnormal' is in the report, and label it accordingly.\n",
        "    if is_abnormal:\n",
        "      predicted_label = 0\n",
        "    elif is_normal:\n",
        "      predicted_label = 1\n",
        "      #print(cleaned_and_decoded_text)\n",
        "      \n",
        "    # If we predicted correctly, add one to our count of correct predictions.\n",
        "    if predicted_label==label_batch[ind]:\n",
        "      n_correct = n_correct+1\n",
        "    else:\n",
        "      # Uncomment the lines below if you want to inspect the cases where we were wrong.\n",
        "      # print(\"--- Wrong example ---\")\n",
        "       print(f\"This example was classified with label {predicted_label} but its actual label is {label_batch[ind].numpy()}.\")\n",
        "       print(\"---\")\n",
        "       if predicted_label == 0:\n",
        "         wrong_abnormal = wrong_abnormal + 1\n",
        "       if predicted_label == 1:\n",
        "         wrong_normal = wrong_normal + 1\n",
        "       \n",
        "       print(cleaned_and_decoded_text)\n",
        "       #print(\"---------------------\")\n",
        "      # pass\n",
        "\n",
        "    # Add one to our count of the total number of examples examined.\n",
        "    n = n+1\n",
        "\n",
        "if count == 0:\n",
        "  print(f\"Accuracy = {round(100*n_correct/n,3)} percent ({n_correct} correct predictions out of {n}). {n - n_correct} misclassified. {wrong_normal} were mislabelled normal while {wrong_abnormal} were mislabelled abnormal\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 100.0 percent (2717 correct predictions out of 2717). 0 misclassified. 0 were mislabelled normal while 0 were mislabelled abnormal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "g6QvDfsSJEwS",
        "outputId": "503a3542-0f82-4992-a1f7-f77592d21cc1"
      },
      "source": [
        "'''\n",
        "BACKUP VERSION\n",
        "\n",
        "TODO:\n",
        "Check labeling.\n",
        "'''\n",
        "\n",
        "'''\n",
        "# First initialise some counters\n",
        "n = 0\n",
        "n_correct = 0\n",
        "n_failed_decode = 0\n",
        "\n",
        "\n",
        "# Iterate over all batches, taking the text and labels batch-by-batch.\n",
        "# N.B. take(-1) has the effect of pulling out all the batches, instead of a specific number, as explained in the docs here: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take\n",
        "for text_batch, label_batch in full_ds.take(-1):\n",
        "\n",
        "  # Iterate over the report examples in the batch:\n",
        "  for ind,text in enumerate(text_batch):\n",
        "\n",
        "    # Get rid of any pesky non-standard characters using the function we created previously.\n",
        "    cleaned_text = clean_text(text,0)\n",
        "    # Then convert it from a tensorflow Tensor to a python string so that we can \n",
        "    # use some standard python text analysis on it.\n",
        "    cleaned_and_decoded_text = cleaned_text.numpy().decode(\"UTF-8\")\n",
        "\n",
        "    find_impression = re.search(\"impression\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\n",
        "    first_char,last_char = find_impression.span()\n",
        "\n",
        "    #find_clinical = re.search(\"clinical correlation:\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\n",
        "    #if find_clinical != None:\n",
        "    #  first_char_clin, last_char_clin = find_clinical.span()\n",
        "    #  last_char = first_char_clin\n",
        "    #else:\n",
        "    last_char = last_char +50\n",
        "\n",
        "    searched_text = cleaned_and_decoded_text.lower()[first_char:last_char]\n",
        "\n",
        "\n",
        "    is_abnormal = re.search('abnormal|absence of normal|outside of the range of normal|not normal', searched_text.lower(), flags=re.IGNORECASE)\n",
        "    #also_abnormal = re.search(\"absence of normal\", searched_text.lower(), flags=re.IGNORECASE)\n",
        "    \n",
        "    # Check if the word 'abnormal' is in the report, and label it accordingly.\n",
        "    if is_abnormal:\n",
        "      predicted_label = 0\n",
        "    else:\n",
        "      predicted_label = 1\n",
        "      \n",
        "    # If we predicted correctly, add one to our count of correct predictions.\n",
        "    if predicted_label==label_batch[ind]:\n",
        "      n_correct = n_correct+1\n",
        "    else:\n",
        "      # Uncomment the lines below if you want to inspect the cases where we were wrong.\n",
        "      # print(\"--- Wrong example ---\")\n",
        "       print(f\"This example was classified with label {predicted_label} but its actual label is {label_batch[ind].numpy()}.\")\n",
        "       print(\"---\")\n",
        "       print(cleaned_and_decoded_text)\n",
        "       print(\"---------------------\")\n",
        "      # pass\n",
        "\n",
        "    # Add one to our count of the total number of examples examined.\n",
        "    n = n+1\n",
        "\n",
        "\n",
        "print(f\"Accuracy = {round(100*n_correct/n,3)} percent ({n_correct} correct predictions out of {n}). {n - n_correct} misclassified.\")\n",
        "'''"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# First initialise some counters\\nn = 0\\nn_correct = 0\\nn_failed_decode = 0\\n\\n\\n# Iterate over all batches, taking the text and labels batch-by-batch.\\n# N.B. take(-1) has the effect of pulling out all the batches, instead of a specific number, as explained in the docs here: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take\\nfor text_batch, label_batch in full_ds.take(-1):\\n\\n  # Iterate over the report examples in the batch:\\n  for ind,text in enumerate(text_batch):\\n\\n    # Get rid of any pesky non-standard characters using the function we created previously.\\n    cleaned_text = clean_text(text,0)\\n    # Then convert it from a tensorflow Tensor to a python string so that we can \\n    # use some standard python text analysis on it.\\n    cleaned_and_decoded_text = cleaned_text.numpy().decode(\"UTF-8\")\\n\\n    find_impression = re.search(\"impression\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\\n    first_char,last_char = find_impression.span()\\n\\n    #find_clinical = re.search(\"clinical correlation:\", cleaned_and_decoded_text.lower(), flags=re.IGNORECASE)\\n    #if find_clinical != None:\\n    #  first_char_clin, last_char_clin = find_clinical.span()\\n    #  last_char = first_char_clin\\n    #else:\\n    last_char = last_char +50\\n\\n    searched_text = cleaned_and_decoded_text.lower()[first_char:last_char]\\n\\n\\n    is_abnormal = re.search(\\'abnormal|absence of normal|outside of the range of normal|not normal\\', searched_text.lower(), flags=re.IGNORECASE)\\n    #also_abnormal = re.search(\"absence of normal\", searched_text.lower(), flags=re.IGNORECASE)\\n    \\n    # Check if the word \\'abnormal\\' is in the report, and label it accordingly.\\n    if is_abnormal:\\n      predicted_label = 0\\n    else:\\n      predicted_label = 1\\n      \\n    # If we predicted correctly, add one to our count of correct predictions.\\n    if predicted_label==label_batch[ind]:\\n      n_correct = n_correct+1\\n    else:\\n      # Uncomment the lines below if you want to inspect the cases where we were wrong.\\n      # print(\"--- Wrong example ---\")\\n       print(f\"This example was classified with label {predicted_label} but its actual label is {label_batch[ind].numpy()}.\")\\n       print(\"---\")\\n       print(cleaned_and_decoded_text)\\n       print(\"---------------------\")\\n      # pass\\n\\n    # Add one to our count of the total number of examples examined.\\n    n = n+1\\n\\n\\nprint(f\"Accuracy = {round(100*n_correct/n,3)} percent ({n_correct} correct predictions out of {n}). {n - n_correct} misclassified.\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuAF8hYfP5Z"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
        "\n",
        "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PabA9DFIfSz7"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8GcJLvb3JH0"
      },
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYGb7z_bfpGm"
      },
      "source": [
        "### Train the model\n",
        "It's time to create our neural network. For the `binary` vectorized data, train a simple bag-of-words linear model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q8iAU-VMzaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273a5aa4-9451-4d77-ec5a-9d7a1a2ce858"
      },
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(2)])\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "118/118 [==============================] - 2s 12ms/step - loss: 0.3358 - accuracy: 0.9126 - val_loss: 0.1709 - val_accuracy: 0.9675\n",
            "Epoch 2/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9825 - val_loss: 0.1119 - val_accuracy: 0.9800\n",
            "Epoch 3/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0839 - accuracy: 0.9910 - val_loss: 0.0860 - val_accuracy: 0.9850\n",
            "Epoch 4/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9952 - val_loss: 0.0709 - val_accuracy: 0.9850\n",
            "Epoch 5/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9979 - val_loss: 0.0609 - val_accuracy: 0.9850\n",
            "Epoch 6/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9984 - val_loss: 0.0538 - val_accuracy: 0.9875\n",
            "Epoch 7/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.9989 - val_loss: 0.0486 - val_accuracy: 0.9875\n",
            "Epoch 8/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9989 - val_loss: 0.0445 - val_accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9989 - val_loss: 0.0413 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9989 - val_loss: 0.0387 - val_accuracy: 0.9900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwidD-SwNIkz"
      },
      "source": [
        "Next, you will use the `int` vectorized layer to build a 1D ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ztw2XH_LbVz"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"elu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9rG1cFRL31Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ff467d-2f92-4905-a43c-f9a695cc15a3"
      },
      "source": [
        "# vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n",
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=2)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "118/118 [==============================] - 4s 26ms/step - loss: 0.4261 - accuracy: 0.8692 - val_loss: 0.0686 - val_accuracy: 0.9875\n",
            "Epoch 2/5\n",
            "118/118 [==============================] - 2s 17ms/step - loss: 0.0343 - accuracy: 0.9915 - val_loss: 0.0237 - val_accuracy: 0.9950\n",
            "Epoch 3/5\n",
            "118/118 [==============================] - 2s 17ms/step - loss: 0.0125 - accuracy: 0.9979 - val_loss: 0.0196 - val_accuracy: 0.9950\n",
            "Epoch 4/5\n",
            "118/118 [==============================] - 2s 17ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.0183 - val_accuracy: 0.9950\n",
            "Epoch 5/5\n",
            "118/118 [==============================] - 2s 17ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3J9Eeuv97zE"
      },
      "source": [
        "Compare the two models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8ViDXw99v_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fdcf7ae-942b-4220-f029-9d38205686ea"
      },
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model on binary vectorized data:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2)                 12414     \n",
            "=================================================================\n",
            "Total params: 12,414\n",
            "Trainable params: 12,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9BOeoCwborD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb83f4e-c52e-43f6-a82f-a0c8a63c48a6"
      },
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet model on int vectorized data:\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 660,738\n",
            "Trainable params: 660,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYW9tUdCtTy"
      },
      "source": [
        "Evaluate both models on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dTc4nZqf7fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddb5503-6bdb-44b7-ac64-d9fd1ce531a6"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27/27 [==============================] - 1s 5ms/step - loss: 0.0478 - accuracy: 0.9907\n",
            "27/27 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9953\n",
            "Binary model accuracy: 99.07%\n",
            "Int model accuracy: 99.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9dhj8Hey9DS"
      },
      "source": [
        "Note: This example dataset represents a rather simple classification problem. More complex datasets and problems bring out subtle but significant differences in preprocessing strategies and model architectures. Be sure to try out different hyperparameters and epochs to compare various approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GaXTsIgP-3"
      },
      "source": [
        "### Export the model\n",
        "\n",
        "In the code above, you applied the `TextVectorization` layer to the dataset before feeding text to the model. If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the `TextVectorization` layer inside your model. To do so, you can create a new model using the weights you just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bRe3KX8gRCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3032c446-190d-4a00-a65a-e941cb764ba3"
      },
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [binary_vectorize_layer, binary_model,\n",
        "     layers.Activation('elu')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_val_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 1s 9ms/step - loss: 0.0087 - accuracy: 0.9950\n",
            "Accuracy: 99.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2eqTVBP4DUN"
      },
      "source": [
        "Now your model can take raw strings as input and predict a score for each label using `model.predict`. Define a function to find the label with the maximum score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU53uRXz45iO"
      },
      "source": [
        "labels = ['abnormal', 'normal']\n",
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = []\n",
        "  for intlab in predicted_int_labels:\n",
        "    predicted_labels.append(labels[intlab.numpy()])\n",
        "  #  predicted_labels = tf.gather(['raw_train_ds.class_names'], predicted_int_labels)\n",
        "  return predicted_labels"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqnWc7Nn5eou"
      },
      "source": [
        "### Run inference on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F-X2ymuDn_M"
      },
      "source": [
        "Now we can create a few custom inputs to explore the model's behaviour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOR2MupW1_zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c236cf-d417-414a-b2c9-f896941882d6"
      },
      "source": [
        "inputs = [\n",
        "    \"This EEG is totally normal\",  # normal\n",
        "    \"This recording is markedly abnormal\",  # abnormal\n",
        "    \"This shows no abnormalities\",  # abnormal\n",
        "    \"Some ever so slight abnormalities, but then again, who can say what normal really means\",  # abnormal\n",
        "    \"They seem fine.\",  # normal?\n",
        "    \"They are fine.\", # normal\n",
        "    \"This person is fine.\",  # normal\n",
        "    \"This person is very unwell.\",  # abnormal\n",
        "    \"IMPRESSION: abnormal\", # abnormal\n",
        "    \"IMPRESSION: markedly abnormal\", # abnormal\n",
        "    \"IMPRESSION: This recording is markedly abnormal\", # abnormal\n",
        "    \"They are not normal\" #abnormal\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "print(predicted_scores)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label, scores in zip(inputs, predicted_labels, predicted_scores):\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)\n",
        "  print(\"Confidence scores: abnormal vs normal\")\n",
        "  print(f\"        {round(scores[0], 2)} vs {round(scores[1], 2)}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.19692296  0.2900898 ]\n",
            " [ 0.5372987  -0.4347239 ]\n",
            " [-0.06418455  0.09787869]\n",
            " [-0.25355804  0.35264894]\n",
            " [ 0.05671321  0.0017204 ]\n",
            " [ 0.02679556  0.0737832 ]\n",
            " [-0.02480018  0.07440683]\n",
            " [ 0.13213836 -0.10222548]\n",
            " [ 0.3157288  -0.308609  ]\n",
            " [ 0.41592282 -0.38063073]\n",
            " [ 0.5337525  -0.43717706]\n",
            " [-0.23353177  0.342391  ]]\n",
            "-----------------------------------\n",
            "Question:  This EEG is totally normal\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        -0.20000000298023224 vs 0.28999999165534973\n",
            "-----------------------------------\n",
            "Question:  This recording is markedly abnormal\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.5400000214576721 vs -0.4300000071525574\n",
            "-----------------------------------\n",
            "Question:  This shows no abnormalities\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        -0.05999999865889549 vs 0.10000000149011612\n",
            "-----------------------------------\n",
            "Question:  Some ever so slight abnormalities, but then again, who can say what normal really means\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        -0.25 vs 0.3499999940395355\n",
            "-----------------------------------\n",
            "Question:  They seem fine.\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.05999999865889549 vs 0.0\n",
            "-----------------------------------\n",
            "Question:  They are fine.\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.029999999329447746 vs 0.07000000029802322\n",
            "-----------------------------------\n",
            "Question:  This person is fine.\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        -0.019999999552965164 vs 0.07000000029802322\n",
            "-----------------------------------\n",
            "Question:  This person is very unwell.\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.12999999523162842 vs -0.10000000149011612\n",
            "-----------------------------------\n",
            "Question:  IMPRESSION: abnormal\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.3199999928474426 vs -0.3100000023841858\n",
            "-----------------------------------\n",
            "Question:  IMPRESSION: markedly abnormal\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.41999998688697815 vs -0.3799999952316284\n",
            "-----------------------------------\n",
            "Question:  IMPRESSION: This recording is markedly abnormal\n",
            "Predicted label:  abnormal\n",
            "Confidence scores: abnormal vs normal\n",
            "        0.5299999713897705 vs -0.4399999976158142\n",
            "-----------------------------------\n",
            "Question:  They are not normal\n",
            "Predicted label:  normal\n",
            "Confidence scores: abnormal vs normal\n",
            "        -0.23000000417232513 vs 0.3400000035762787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDVfii_4slI"
      },
      "source": [
        "Including the text preprocessing logic inside your model enables you to export a model for production that simplifies deployment, and reduces the potential for [train/test skew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew).\n",
        "\n",
        "There is a performance difference to keep in mind when choosing where to apply your `TextVectorization` layer. Using it outside of your model enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So, if you're training your model on the GPU, you probably want to go with this option to get the best performance while developing your model, then switch to including the TextVectorization layer inside your model when you're ready to prepare for deployment.\n",
        "\n",
        "Visit this [tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load) to learn more about saving models."
      ]
    }
  ]
}